Full-blown GATK pipeline for 2bRAD data, based on bowtie2 mapping to a reference genome
September 14, 2014, Mikhail Matz matz@utexas.edu

# setting up the genome reference

# concatenating genome contigs into small number of "pseudo-chromosomes" (to help memory usage in GATK).
# Using the scaffolded Idotea assembly which has been run through the redundans pipeline:

~/scripts/concatFasta.pl fasta=Idotea_0002_reduced_pruned_arcs_scaffold.fa

# normalize fasta records to ensure all lines are of the same length, using Picard

module load picard/v2.1.1

java -Xmx1g -jar ~/scripts/NormalizeFasta.jar INPUT=Idotea_0002_reduced_pruned_arcs_scaffold_cc.fasta OUTPUT=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

# One possibility is to edit the env variables to make the software easier find the genome and software.
# If so, edit these lines THROUGHOUT THE TEXT to fit the location and name of your genome reference 
#export GENOME_FASTA=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta
#export GENOME_DICT=Idotea_0002_reduced_ccn.dict
#export GENOME_PATH=/nobackup/data7/pierre/IB14/
#export GENOME_REF=/nobackup/data7/pierre/IB14/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta
#export TACC_GATK_DIR=~/scripts/
#export TACC_PICARD_DIR=~/scripts/


module load Bowtie2/v2.2.7

# creating genome indexes:
bowtie2-build Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

module load samtools
samtools faidx Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

java -jar ~/scripts/CreateSequenceDictionary.jar R=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta  O=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.dict

#----------------------------------
# concatenating the reads files (make sure to unzip them first!):

~/scripts/ngs_concat.pl fastq "IB14(.+)_S\d+"

# Trimming the reads:

~/scripts/2bRAD_trim_launch.pl fq > trims.sh

#Then open the file and make sure the path to 2bRADtrim.pl is correct, and add a header line #!/bin/bash

chmod 755 trims.sh
./trims.sh

# quality filtering using fastx_toolkit
module load fastx_toolkit
ls *.tr0 | perl -pe 's/^(\S+)\.tr0$/cat $1\.tr0 \| fastq_quality_filter -q 20 -p 90 >$1\.trim/' >filt0
cat filt0 | perl -pe 's/filter /filter -Q33 /' > filt.sh

#Then open the file and add a header line #!/bin/bash

chmod 755 filt.sh
./filt.sh

#----------------------------------
# mapping reads and reformatting mapped data files

# aligning with bowtie2 :

~/scripts/2bRAD_bowtie2_launch.pl '\.trim$' /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta > bt2_aln.sge

# You can add a multithreading flag (-p 16) to the sge script with:

sed -i 's/-L 16/-L 16 -p 16/g' bt2.sge

#Then open the file and make sure the path is correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 16
module load Bowtie2/v2.2.7

# Re-save the file, then submit the job to the high_mem queue:

qsub bt2.sge

ls *.bt2.sam > sams
cat sams | wc -l  
# do you have sams for all your samples?... If not, rerun the chunk above

# making bam files

~/scripts/AddReadGroups.sh
~/scripts/convert_to_bam.sh

#rm *sorted*
#ls *bt2.bam > bams
#cat bams | wc -l  
# do you have bams for all your samples?... If not, rerun the chunk above

#----------------------------------
# WARNING!!! DO NOT RUN THIS FOR RAD!!! SKIPPING THIS SECTION!
# marking duplicate reads 
# module load picard
# export GENOME_REF=where/genome/is/mygenome_ccn.fasta
#cat bams | perl -pe 's/(\S+)\.bam/java -Xmx4g -jar ~\/scripts\/MarkDuplicates\.jar INPUT=$1\.bam OUTPUT=$1\.dedup.bam METRICS_FILE=$1\.metrics AS=true CREATE_INDEX=true/' > dd
#launcher_creator.py -j dd -n dd -l dd.job -q normal
#cat dd.job | perl -pe 's/12way \d+/4way NNN/' > ddup.job  # REPLACE NNN WITH 12*ceiling([number of bam files]/4)
#qsub ddup.job
#ls *.dedup.bam > bams

#----------------------------------
# starting GATK
# realigning around indels:

# step one: finding places to realign:

#First, index all the bam files with samtools index:

~/scripts/indexBams.sh

#Prep the input file for RealignerTargetCreator:

ls *.bam > bams
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T RealignerTargetCreator -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -I $1\.bam -o $1\.intervals/' >intervals.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

qsub intervals.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.intervals | wc -l

# step two: realigning
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T IndelRealigner -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -targetIntervals $1\.intervals -I $1\.bam -o $1\.real.bam -LOD 0\.4/' >realign.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

qsub realign.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l

#----------------------------------
# launching GATK UnifiedGenotyper for round 1 (about 30 min)
# note: it is a preliminary run needed for base quality recalibration,

#Steo one - merge all the bam files into one file called merged.bam, while keeping the sample information as "read groups" 
# First - create an rg.txt file, consisting of one line per file 
# @RG	ID:SAMPLE.fq.trim	SM:SAMPLE.fq.trim	PL:Illumina
# then merge all the bam files:

samtools merge -h rg.txt merged.bam *.real.bam

## and index the merged file:

samtools index merged.bam

# Then, create the round 1 submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -nt 12 -nct 1 \
-I merged.bam \
-o round1.vcf' >unig.sge

# And submit it with:

qsub unig.sge

#----------------------------------
# base quality score recalibration (BQSR)

# creating high-confidence (>75 quality percentile) snp sets for 
# base quality recalibration using Kyle Hernandez's tool. (ignore the warnings)
~/scripts/GetHighQualVcfs.py  -i round1.vcf --percentile 75 -o .

# recalibrating quality scores
# step one: creating recalibration reports

ls *real.bam > bams
cat bams | perl -pe 's/(\S+)\.real\.bam/java -Xmx20g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T BaseRecalibrator -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -knownSites $1\.fq\.trim_HQ\.vcf -I $1\.real\.bam -o $1\.real\.recalibration_report.grp/' >bqsr.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:
qsub bqsr.sge

# did it run for all files? is the number of *.grp files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.grp | wc -l

# step two: rewriting bams according to recalibration reports

cat bams | perl -pe 's/(\S+)\.bam/java -Xmx10g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T PrintReads -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -I $1\.bam -BQSR $1\.recalibration_report.grp -o $1\.recal\.bam /' >bqsr2.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:

qsub bqsr2.sge

# did it run for all files? is the number of *.recal.bam files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.recal.bam | wc -l

ls *.recal.bam > bams

#----------------------------------
# Second iteration of UnifiedGenotyper (on quality-recalibrated files)
# this time FOR REAL! 
# Mischa's comment below::
# if you need indels, run the same process separately with --genotype_likelihoods_model INDEL
# I do not recommend indel tracing for 2bRAD since the tags are too short for confident indels. 
# If you still want to try, note that the subsequent recalibration stages would 
# have do be done separately for indels, 

samtools merge -h rg.txt merged_recalibrated.bam *.recal.bam

## and index the merged file:

samtools index merged_recalibrated.bam

# Then, create the submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -nt 12 -nct 1 \
--genotype_likelihoods_model SNP \
-I merged_recalibrated.bam \
-o round2.vcf' >unig2.sge

qsub unig2.sge

# renaming samples in the vcf file, to get rid of trim-shmim etc
cat round2.vcf | perl -pe 's/\.fq\.trim//g' | perl -pe 's/^chrom/chr/' >round2.names.vcf

#----------------------------------
# variant quality score recalibration (VQSR)

# extracting SNPs that are consistently genotyped in replicates 
# and have not too many heterozygotes:
~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab >vqsr.vcf

# determining transition-transversion ratio for true snps (will need it for tranche calibration)
vcftools --vcf vqsr.vcf --TsTv-summary
# Ts/Tv ratio: 1.379  # put your actual number into the next code chunk, --target_titv

# Recalibrating genotype calls: VQSR
# step one - creating recalibration models (30 sec)

java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T VariantRecalibrator \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -input round2.names.vcf -nt 12 \
-resource:repmatch,known=true,training=true,truth=true,prior=10  vqsr.vcf \
-an QD -an MQ -an FS -mode SNP --maxGaussians 4 \
--target_titv 1.379 -tranche 85.0 -tranche 90.0 -tranche 95.0 -tranche 99.0 -tranche 100 \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -rscriptFile recalibrate_SNP_plots.R 

# fixing the R script (assumes outdated version of ggplot2):
cat recalibrate_SNP_plots.R | perl -pe 's/opts\(/theme\(/'g | perl -pe 's/theme_/element_/g' | perl -pe 's/\+ theme\(title=\"model PDF\"\)//g'  >recalibrateSNPs.R
# now copy all recalibrate* files to your laptop, run the R script, examine the resulting plot and tranches.pdf

# applying recalibration:
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T ApplyRecalibration \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -input round2.names.vcf -nt 12 \
--ts_filter_level 95.0 -mode SNP \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -o gatk_after_vqsr.vcf

#----------------------------------
# ALTERNATIVE to GATK-based recalibration (if tranches are weird and error models fail to converge):
# non-parametric quantile-based recalibration a-la de novo pipeline

~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab polyonly=1 >vqsr.vcf
~/scripts/recalibrateSNPs_gatk.pl vcf=round2.names.vcf true=vqsr.vcf >gatk_after_vqsr.vcf

# your best quality filter setting is the one with maximum "gain"

#----------------------------------

# restoring original contig names and coordinates:
~/scripts/retabvcf.pl vcf=gatk_after_vqsr.vcf tab=/nobackup/data7/pierre/IBFull-blown GATK pipeline for 2bRAD data, based on bowtie2 mapping to a reference genome
September 14, 2014, Mikhail Matz matz@utexas.edu

# setting up the genome reference

# concatenating genome contigs into small number of "pseudo-chromosomes" (to help memory usage in GATK).
# Using the scaffolded Idotea assembly which has been run through the redundans pipeline:

~/scripts/concatFasta.pl fasta=Idotea_0002_reduced_pruned_arcs_scaffold.fa

# normalize fasta records to ensure all lines are of the same length, using Picard

module load picard/v2.1.1

java -Xmx1g -jar ~/scripts/NormalizeFasta.jar INPUT=Idotea_0002_reduced_pruned_arcs_scaffold_cc.fasta OUTPUT=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

# One possibility is to edit the env variables to make the software easier find the genome and software.
# If so, edit these lines THROUGHOUT THE TEXT to fit the location and name of your genome reference 
#export GENOME_FASTA=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta
#export GENOME_DICT=Idotea_0002_reduced_ccn.dict
#export GENOME_PATH=/nobackup/data7/pierre/IB14/
#export GENOME_REF=/nobackup/data7/pierre/IB14/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta
#export TACC_GATK_DIR=~/scripts/
#export TACC_PICARD_DIR=~/scripts/


module load Bowtie2/v2.2.7

# creating genome indexes:
bowtie2-build Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

module load samtools
samtools faidx Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta

java -jar ~/scripts/CreateSequenceDictionary.jar R=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta  O=Idotea_0002_reduced_pruned_arcs_scaffold_ccn.dict

#----------------------------------
# concatenating the reads files (make sure to unzip them first!):

~/scripts/ngs_concat.pl fastq "IB14(.+)_S\d+"

# Trimming the reads:

~/scripts/2bRAD_trim_launch.pl fq > trims.sh

#Then open the file and make sure the path to 2bRADtrim.pl is correct, and add a header line #!/bin/bash

chmod 755 trims.sh
./trims.sh

# quality filtering using fastx_toolkit
module load fastx_toolkit
ls *.tr0 | perl -pe 's/^(\S+)\.tr0$/cat $1\.tr0 \| fastq_quality_filter -q 20 -p 90 >$1\.trim/' >filt0
cat filt0 | perl -pe 's/filter /filter -Q33 /' > filt.sh

#Then open the file and add a header line #!/bin/bash

chmod 755 filt.sh
./filt.sh

#----------------------------------
# mapping reads and reformatting mapped data files

# aligning with bowtie2 :

~/scripts/2bRAD_bowtie2_launch.pl '\.trim$' /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta > bt2_aln.sge

# You can add a multithreading flag (-p 16) to the sge script with:

sed -i 's/-L 16/-L 16 -p 16/g' bt2.sge

#Then open the file and make sure the path is correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 16
module load Bowtie2/v2.2.7

# Re-save the file, then submit the job to the high_mem queue:

qsub bt2.sge

ls *.bt2.sam > sams
cat sams | wc -l  
# do you have sams for all your samples?... If not, rerun the chunk above

# making bam files

~/scripts/AddReadGroups.sh
~/scripts/convert_to_bam.sh

#rm *sorted*
#ls *bt2.bam > bams
#cat bams | wc -l  
# do you have bams for all your samples?... If not, rerun the chunk above

#----------------------------------
# WARNING!!! DO NOT RUN THIS FOR RAD!!! SKIPPING THIS SECTION!
# marking duplicate reads 
# module load picard
# export GENOME_REF=where/genome/is/mygenome_ccn.fasta
#cat bams | perl -pe 's/(\S+)\.bam/java -Xmx4g -jar ~\/scripts\/MarkDuplicates\.jar INPUT=$1\.bam OUTPUT=$1\.dedup.bam METRICS_FILE=$1\.metrics AS=true CREATE_INDEX=true/' > dd
#launcher_creator.py -j dd -n dd -l dd.job -q normal
#cat dd.job | perl -pe 's/12way \d+/4way NNN/' > ddup.job  # REPLACE NNN WITH 12*ceiling([number of bam files]/4)
#qsub ddup.job
#ls *.dedup.bam > bams

#----------------------------------
# starting GATK
# realigning around indels:

# step one: finding places to realign:

#First, index all the bam files with samtools index:

~/scripts/indexBams.sh

#Prep the input file for RealignerTargetCreator:

ls *.bam > bams
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T RealignerTargetCreator -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -I $1\.bam -o $1\.intervals/' >intervals.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

qsub intervals.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.intervals | wc -l

# step two: realigning
cat bams | perl -pe 's/(\S+)\.bam/java -Xmx5g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T IndelRealigner -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -targetIntervals $1\.intervals -I $1\.bam -o $1\.real.bam -LOD 0\.4/' >realign.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

qsub realign.sge

# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l

#----------------------------------
# launching GATK UnifiedGenotyper for round 1 (about 30 min)
# note: it is a preliminary run needed for base quality recalibration,

#Steo one - merge all the bam files into one file called merged.bam, while keeping the sample information as "read groups" 
# First - create an rg.txt file, consisting of one line per file 
# @RG	ID:SAMPLE.fq.trim	SM:SAMPLE.fq.trim	PL:Illumina
# then merge all the bam files:

samtools merge -h rg.txt merged.bam *.real.bam

## and index the merged file:

samtools index merged.bam

# Then, create the round 1 submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -nt 12 -nct 1 \
-I merged.bam \
-o round1.vcf' >unig.sge

# And submit it with:

qsub unig.sge

#----------------------------------
# base quality score recalibration (BQSR)

# creating high-confidence (>75 quality percentile) snp sets for 
# base quality recalibration using Kyle Hernandez's tool. (ignore the warnings)
~/scripts/GetHighQualVcfs.py  -i round1.vcf --percentile 75 -o .

# recalibrating quality scores
# step one: creating recalibration reports

ls *real.bam > bams
cat bams | perl -pe 's/(\S+)\.real\.bam/java -Xmx20g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T BaseRecalibrator -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -knownSites $1\.fq\.trim_HQ\.vcf -I $1\.real\.bam -o $1\.real\.recalibration_report.grp/' >bqsr.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:
qsub bqsr.sge

# did it run for all files? is the number of *.grp files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.grp | wc -l

# step two: rewriting bams according to recalibration reports

cat bams | perl -pe 's/(\S+)\.bam/java -Xmx10g -jar ~\/scripts\/GenomeAnalysisTK_3_6\.jar -T PrintReads -R \/nobackup\/data7\/pierre\/IB14\/Arcs_GATK\/Idotea_0002_reduced_pruned_arcs_scaffold_ccn\.fasta -I $1\.bam -BQSR $1\.recalibration_report.grp -o $1\.recal\.bam /' >bqsr2.sge

#Then open the file and make sure the paths are correct, and add a header:
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 1

# And submit the sge script with:

qsub bqsr2.sge

# did it run for all files? is the number of *.recal.bam files equal the number of *.real.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l
ll *.recal.bam | wc -l

ls *.recal.bam > bams

#----------------------------------
# Second iteration of UnifiedGenotyper (on quality-recalibrated files)
# this time FOR REAL! 
# Mischa's comment below::
# if you need indels, run the same process separately with --genotype_likelihoods_model INDEL
# I do not recommend indel tracing for 2bRAD since the tags are too short for confident indels. 
# If you still want to try, note that the subsequent recalibration stages would 
# have do be done separately for indels, 

samtools merge -h rg.txt merged_recalibrated.bam *.recal.bam

## and index the merged file:

samtools index merged_recalibrated.bam

# Then, create the submission script with:

echo '#!/bin/bash
#$ -cwd
#$ -q high_mem
#$ -S /bin/bash
#$ -pe mpich 12
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T UnifiedGenotyper \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -nt 12 -nct 1 \
--genotype_likelihoods_model SNP \
-I merged_recalibrated.bam \
-o round2.vcf' >unig2.sge

qsub unig2.sge

# renaming samples in the vcf file, to get rid of trim-shmim etc
cat round2.vcf | perl -pe 's/\.fq\.trim//g' | perl -pe 's/^chrom/chr/' >round2.names.vcf

#----------------------------------
# variant quality score recalibration (VQSR)

# extracting SNPs that are consistently genotyped in replicates 
# and have not too many heterozygotes:
~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab >vqsr.vcf

# determining transition-transversion ratio for true snps (will need it for tranche calibration)
vcftools --vcf vqsr.vcf --TsTv-summary
# Ts/Tv ratio: 1.379  # put your actual number into the next code chunk, --target_titv

# Recalibrating genotype calls: VQSR
# step one - creating recalibration models (30 sec)

java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T VariantRecalibrator \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -input round2.names.vcf -nt 12 \
-resource:repmatch,known=true,training=true,truth=true,prior=10  vqsr.vcf \
-an QD -an MQ -an FS -mode SNP --maxGaussians 4 \
--target_titv 1.379 -tranche 85.0 -tranche 90.0 -tranche 95.0 -tranche 99.0 -tranche 100 \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -rscriptFile recalibrate_SNP_plots.R 

# fixing the R script (assumes outdated version of ggplot2):
cat recalibrate_SNP_plots.R | perl -pe 's/opts\(/theme\(/'g | perl -pe 's/theme_/element_/g' | perl -pe 's/\+ theme\(title=\"model PDF\"\)//g'  >recalibrateSNPs.R
# now copy all recalibrate* files to your laptop, run the R script, examine the resulting plot and tranches.pdf

# applying recalibration:
java -jar ~/scripts/GenomeAnalysisTK_3_6.jar -T ApplyRecalibration \
-R /nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_ccn.fasta -input round2.names.vcf -nt 12 \
--ts_filter_level 95.0 -mode SNP \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -o gatk_after_vqsr.vcf

#----------------------------------
# ALTERNATIVE to GATK-based recalibration (if tranches are weird and error models fail to converge):
# non-parametric quantile-based recalibration a-la de novo pipeline

~/scripts/replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab polyonly=1 >vqsr.vcf
~/scripts/recalibrateSNPs_gatk.pl vcf=round2.names.vcf true=vqsr.vcf >gatk_after_vqsr.vcf

# your best quality filter setting is the one with maximum "gain"

#----------------------------------

# restoring original contig names and coordinates:
~/scripts/retabvcf.pl vcf=gatk_after_vqsr.vcf tab=/nobackup/data7/pierre/IB14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_cc.tab > retab.vcf

RAN THE PIPELINE TO HERE!!!

# discarding loci with too many heterozygotes, which are likely lumped paralogs
# (by default, fraction of heterozygotes should not exceed maxhet=0.75)
# this step can also filter for the fraction of missing genotypes (default maxmiss=0.5)
~/scripts/hetfilter.pl vcf=retab.vcf >hetfilt.vcf

# thinning SNP dataset - leaving one snp per tag
# by default, it will leave the SNP with the highest minor allele frequency; this
# is good for ADMIXTURE or Fst analysis
# if you plan to use dadi, however, use it with the option criterion=maxDP-random
~/scripts/thinner.pl vcf=hetfilt.vcf > thin.vcf

# applying filter and selecting polymorphic biallelic loci genotyped in 80% or more individuals
# for parametric (GATK-based) recalibration"
vcftools --vcf thin.vcf --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0
#After filtering, kept 40460 out of a possible 73062 Sites

# for non parametric (GATK-based) recalibration: replace --minQ 15 in the following line
# with the quantile of the highest "gain" as reported by recalibrateSNPs_gatk.pl
#vcftools --vcf thin.vcf --minQ 15 --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0

# genotypic match between pairs of replicates 
# (the most important one is the last one, Heterozygote Discovery Rate)	
~/scripts/repMatchStats.pl vcf=filt0.recode.vcf replicates=clonepairs.tab 
# Saved the output as repmatchstats.txt!

# creating final filtered file without clones (must list them in the file clones2remove):
# (for non-parametric recalibration, replace --remove-filtered-all with --minQ [quantile of the highest gain] )
vcftools --vcf thin.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out final

# After filtering, kept 622 out of 744 Individuals
# After filtering, kept 34709 out of a possible 55963 Sites

#Finally, output all kinds of statistics with vcftools:
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --hardy
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --012
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --site-mean-depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --het
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --missing-indv





FINISHED THE PIPELINE TO HERE!!!



---------------------------------------------------

# creating unthinned dataset for Tajima's D calculations
# (for non-parametric recalibration, replace --remove-filtered-all with --minQ [quantile of the highest gain] )
# vcftools --vcf hetfilt.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out finalUnthin


# ===================================================
# Analysis

# creating population lists
# in the example below all the files beginning with O are from one pop, K from another, so
ls K*.recal.bam | perl -pe 's/\..+//' >K.pop
ls O*.recal.bam | perl -pe 's/\..+//' >O.pop
cat O.pop K.pop  | perl -pe 's/((.).+)/$1\t$2/' >OK.pops

#-------------
# need PGDspider - the format-converting software!
# install PDGspider on your laptop: 
# http://www.cmpg.unibe.ch/software/PGDSpider/#Download_and_Installation_Instructions  

# install PGDspider on lonestar:
cd ~/bin
wget http://www.cmpg.unibe.ch/software/PGDSpider/PGDSpider_2.0.7.1.zip
unzip PGDSpider_2.0.7.1.zip
cd -

#-------------
# BayeScan: searching for Fst outliers

# converting vcf to bayescan format

# creating configuration file for PDGspider
nano vcf2bayescan.spid

# paste this (edit if needed):
####################################
# VCF Parser questions
PARSER_FORMAT=VCF
# Do you want to include a file with population definitions?
VCF_PARSER_POP_QUESTION=true
# Only input following regions (refSeqName:start:end, multiple regions: whitespace separated):
VCF_PARSER_REGION_QUESTION=
# What is the ploidy of the data?
VCF_PARSER_PLOIDY_QUESTION=DIPLOID
# Only output following individuals (ind1, ind2, ind4, ...):
VCF_PARSER_IND_QUESTION=
# Output genotypes as missing if the read depth of a position for the sample is below:
VCF_PARSER_READ_QUESTION=
# Take most likely genotype if "PL" or "GL" is given in the genotype field?
VCF_PARSER_PL_QUESTION=true
# Do you want to exclude loci with only missing data?
VCF_PARSER_EXC_MISSING_LOCI_QUESTION=false
# Select population definition file:
VCF_PARSER_POP_FILE_QUESTION=./OK.pops
# Only output SNPs with a phred-scaled quality of at least:
VCF_PARSER_QUAL_QUESTION=
# Do you want to include non-polymorphic SNPs?
VCF_PARSER_MONOMORPHIC_QUESTION=false
# Output genotypes as missing if the phred-scale genotype quality is below:
VCF_PARSER_GTQUAL_QUESTION=

# GESTE / BayeScan Writer questions
WRITER_FORMAT=GESTE_BAYE_SCAN
# Specify which data type should be included in the GESTE / BayeScan file  (GESTE / BayeScan can only analyze one data type per file):
GESTE_BAYE_SCAN_WRITER_DATA_TYPE_QUESTION=SNP
####################################

java -Xmx1024m -Xms512m -jar ~/bin/PGDSpider_2.0.7.1/PGDSpider2-cli.jar -inputfile final.recode.vcf -outputfile OK.bayescan -spid vcf2bayescan.spid 

# Download and install BayeScan 
cd ~/bin
wget http://cmpg.unibe.ch/software/BayeScan/files/BayeScan2.1.zip
unzip BayeScan2.1.zip
cp BayeScan2.1/binaries/BayeScan2.1_linux64bits bayescan
chmod +x bayescan
rm -r BayeScan*

echo 'bayescan OK.bayescan -threads=192' >bs
launcher_creator.py -j bs -n bs -l bsl
cat bsl | perl -pe 's/12way 12/12way 192/' | perl -pe 's/h_rt=1/h_rt=24/' |  perl -pe 's/development/normal/' >bsll
qsub bsll
# this one takes a few hours

#-------------
# pi: (nucleotide diversity = expected heterozygosity)

vcftools --vcf final.recode.vcf --keep O.pop --site-pi
mv out.sites.pi O.pi
vcftools --vcf final.recode.vcf  --keep K.pop --site-pi
mv out.sites.pi K.pi

#-------------
# Fst:

vcftools --vcf final.recode.vcf  --weir-fst-pop O.pop --weir-fst-pop K.pop  
# 0.011437
mv out.weir.fst OK.fst

#-------------
# Tajima's D (separately for two pops):

vcftools --vcf finalUnthin.recode.vcf  --keep K.pop --TajimaD 75000
mv out.Tajima.D K.td
vcftools --vcf finalUnthin.recode.vcf  --keep O.pop --TajimaD 75000
mv out.Tajima.D O.td

#-------------
# LD (using only bi-allelic sites within 15 kb of each other) - these two jobs take quite some time...
echo 'vcftools --vcf final.recode.vcf --geno-r2 --ld-window-bp 15000' >ld
launcher_creator.py -j ld -n ld -l ldj
cat ldj | perl -pe 's/12way/1way/' | perl -pe 's/h_rt=1/h_rt=12/' |perl -pe 's/development/normal/' > ldjj
qsub ldjj

# wait for it to complete (~15 min), then do this (another 15-20 min):
module load R
echo 'Rscript ~/bin/analyzeLD.R' > ldr
launcher_creator.py -j ldr -n ldr -l ldrj
cat ldrj | perl -pe 's/12way/1way/' | perl -pe 's/h_rt=1/h_rt=24/' |perl -pe 's/development/normal/' > ldrjj
qsub ldrjj

# scp (or WinSCP) the resulting files: OK.baye_fst.txt, O.pi, K.pi, OK.fst, K.td, O.td, LD_quantized.RData 
# to your laptop

# use genomeScanPlots.R to plot it

#-------------
# ADMIXTURE
#-------------

# installing ADMIXTURE
cd ~/bin/
wget https://www.genetics.ucla.edu/software/admixture/binaries/admixture_linux-1.23.tar.gz --no-check-certificate
tar vxf admixture_linux-1.23.tar.gz 
mv admixture_linux-1.23/admixture .
cd -

# installing plink 1.09:
cd ~/bin
wget https://www.cog-genomics.org/static/bin/plink140918/plink_linux_x86_64.zip
unzip plink_linux_x86_64.zip
cd-

# for denovo vcf, creating a dataset with fake "chr" chromosome designations
cat denovo.vcf | perl -pe 's/locus(\d)(\d+)\t\d+/chr$1\t$2/' >chrom.recode.vcf

# for GATK-based vcf, creating a high-thinned but not re-tabbed dataset with "chr" chromosome designations
hetfilter.pl vcf=gatk_after_vqsr.vcf >hetfiltRaw.vcf
thinner.pl vcf=hetfiltRaw.vcf interval=5000 | perl -pe 's/chrom/chr/g' >thinchrom.vcf
vcftools --vcf thinchrom.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --out chrom

# reformatting VCF into plink binary BED format
plink --vcf chrom.recode.vcf --make-bed --out admix

# running ADMIXTURE with
# cross-validation to select K (bash script)
for K in 1 2 3 4 5 6; \
do admixture --cv admix.bed $K | tee log${K}.out; done

grep -h CV log*.out # select K with the minimal value

# listing individuals from vcf file
grep "#CHROM" chrom.recode.vcf | perl -pe 's/\t/\n/g' | grep [0-9] > inds.list

# scp the *.Q and inds.list files to laptop, plot it in R:
# use admixturePlotting.R to plot (will require minor editing - population names)14/Arcs_GATK/Idotea_0002_reduced_pruned_arcs_scaffold_cc.tab > retab.vcf

# discarding loci with too many heterozygotes, which are likely lumped paralogs
# (by default, fraction of heterozygotes should not exceed maxhet=0.75)
# this step can also filter for the fraction of missing genotypes (default maxmiss=0.5)
~/scripts/hetfilter.pl vcf=retab.vcf >hetfilt.vcf

# thinning SNP dataset - leaving one snp per tag
# by default, it will leave the SNP with the highest minor allele frequency; this
# is good for ADMIXTURE or Fst analysis
# if you plan to use dadi, however, use it with the option criterion=maxDP-random
~/scripts/thinner.pl vcf=hetfilt.vcf > thin.vcf

# applying filter and selecting polymorphic biallelic loci genotyped in 80% or more individuals
# for parametric (GATK-based) recalibration"
vcftools --vcf thin.vcf --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0
#After filtering, kept 40460 out of a possible 73062 Sites

# for non parametric (GATK-based) recalibration: replace --minQ 15 in the following line
# with the quantile of the highest "gain" as reported by recalibrateSNPs_gatk.pl
#vcftools --vcf thin.vcf --minQ 15 --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0

# genotypic match between pairs of replicates 
# (the most important one is the last one, Heterozygote Discovery Rate)	
~/scripts/repMatchStats.pl vcf=filt0.recode.vcf replicates=clonepairs.tab 
# Saved the output as repmatchstats.txt!

# creating final filtered file without clones (must list them in the file clones2remove):
# (for non-parametric recalibration, replace --remove-filtered-all with --minQ [quantile of the highest gain] )
vcftools --vcf thin.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out final

# After filtering, kept 622 out of 744 Individuals
# After filtering, kept 34709 out of a possible 55963 Sites

#Finally, output all kinds of statistics with vcftools:
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --hardy
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --012
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --site-mean-depth
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --het
vcftools --vcf final.recode.vcf --out IB14_Arcs_GATK_v1_SNPs --missing-indv





FINISHED THE PIPELINE TO HERE!!!



---------------------------------------------------

# creating unthinned dataset for Tajima's D calculations
# (for non-parametric recalibration, replace --remove-filtered-all with --minQ [quantile of the highest gain] )
# vcftools --vcf hetfilt.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out finalUnthin


# ===================================================
# Analysis

# creating population lists
# in the example below all the files beginning with O are from one pop, K from another, so
ls K*.recal.bam | perl -pe 's/\..+//' >K.pop
ls O*.recal.bam | perl -pe 's/\..+//' >O.pop
cat O.pop K.pop  | perl -pe 's/((.).+)/$1\t$2/' >OK.pops

#-------------
# need PGDspider - the format-converting software!
# install PDGspider on your laptop: 
# http://www.cmpg.unibe.ch/software/PGDSpider/#Download_and_Installation_Instructions  

# install PGDspider on lonestar:
cd ~/bin
wget http://www.cmpg.unibe.ch/software/PGDSpider/PGDSpider_2.0.7.1.zip
unzip PGDSpider_2.0.7.1.zip
cd -

#-------------
# BayeScan: searching for Fst outliers

# converting vcf to bayescan format

# creating configuration file for PDGspider
nano vcf2bayescan.spid

# paste this (edit if needed):
####################################
# VCF Parser questions
PARSER_FORMAT=VCF
# Do you want to include a file with population definitions?
VCF_PARSER_POP_QUESTION=true
# Only input following regions (refSeqName:start:end, multiple regions: whitespace separated):
VCF_PARSER_REGION_QUESTION=
# What is the ploidy of the data?
VCF_PARSER_PLOIDY_QUESTION=DIPLOID
# Only output following individuals (ind1, ind2, ind4, ...):
VCF_PARSER_IND_QUESTION=
# Output genotypes as missing if the read depth of a position for the sample is below:
VCF_PARSER_READ_QUESTION=
# Take most likely genotype if "PL" or "GL" is given in the genotype field?
VCF_PARSER_PL_QUESTION=true
# Do you want to exclude loci with only missing data?
VCF_PARSER_EXC_MISSING_LOCI_QUESTION=false
# Select population definition file:
VCF_PARSER_POP_FILE_QUESTION=./OK.pops
# Only output SNPs with a phred-scaled quality of at least:
VCF_PARSER_QUAL_QUESTION=
# Do you want to include non-polymorphic SNPs?
VCF_PARSER_MONOMORPHIC_QUESTION=false
# Output genotypes as missing if the phred-scale genotype quality is below:
VCF_PARSER_GTQUAL_QUESTION=

# GESTE / BayeScan Writer questions
WRITER_FORMAT=GESTE_BAYE_SCAN
# Specify which data type should be included in the GESTE / BayeScan file  (GESTE / BayeScan can only analyze one data type per file):
GESTE_BAYE_SCAN_WRITER_DATA_TYPE_QUESTION=SNP
####################################

java -Xmx1024m -Xms512m -jar ~/bin/PGDSpider_2.0.7.1/PGDSpider2-cli.jar -inputfile final.recode.vcf -outputfile OK.bayescan -spid vcf2bayescan.spid 

# Download and install BayeScan 
cd ~/bin
wget http://cmpg.unibe.ch/software/BayeScan/files/BayeScan2.1.zip
unzip BayeScan2.1.zip
cp BayeScan2.1/binaries/BayeScan2.1_linux64bits bayescan
chmod +x bayescan
rm -r BayeScan*

echo 'bayescan OK.bayescan -threads=192' >bs
launcher_creator.py -j bs -n bs -l bsl
cat bsl | perl -pe 's/12way 12/12way 192/' | perl -pe 's/h_rt=1/h_rt=24/' |  perl -pe 's/development/normal/' >bsll
qsub bsll
# this one takes a few hours

#-------------
# pi: (nucleotide diversity = expected heterozygosity)

vcftools --vcf final.recode.vcf --keep O.pop --site-pi
mv out.sites.pi O.pi
vcftools --vcf final.recode.vcf  --keep K.pop --site-pi
mv out.sites.pi K.pi

#-------------
# Fst:

vcftools --vcf final.recode.vcf  --weir-fst-pop O.pop --weir-fst-pop K.pop  
# 0.011437
mv out.weir.fst OK.fst

#-------------
# Tajima's D (separately for two pops):

vcftools --vcf finalUnthin.recode.vcf  --keep K.pop --TajimaD 75000
mv out.Tajima.D K.td
vcftools --vcf finalUnthin.recode.vcf  --keep O.pop --TajimaD 75000
mv out.Tajima.D O.td

#-------------
# LD (using only bi-allelic sites within 15 kb of each other) - these two jobs take quite some time...
echo 'vcftools --vcf final.recode.vcf --geno-r2 --ld-window-bp 15000' >ld
launcher_creator.py -j ld -n ld -l ldj
cat ldj | perl -pe 's/12way/1way/' | perl -pe 's/h_rt=1/h_rt=12/' |perl -pe 's/development/normal/' > ldjj
qsub ldjj

# wait for it to complete (~15 min), then do this (another 15-20 min):
module load R
echo 'Rscript ~/bin/analyzeLD.R' > ldr
launcher_creator.py -j ldr -n ldr -l ldrj
cat ldrj | perl -pe 's/12way/1way/' | perl -pe 's/h_rt=1/h_rt=24/' |perl -pe 's/development/normal/' > ldrjj
qsub ldrjj

# scp (or WinSCP) the resulting files: OK.baye_fst.txt, O.pi, K.pi, OK.fst, K.td, O.td, LD_quantized.RData 
# to your laptop

# use genomeScanPlots.R to plot it

#-------------
# ADMIXTURE
#-------------

# installing ADMIXTURE
cd ~/bin/
wget https://www.genetics.ucla.edu/software/admixture/binaries/admixture_linux-1.23.tar.gz --no-check-certificate
tar vxf admixture_linux-1.23.tar.gz 
mv admixture_linux-1.23/admixture .
cd -

# installing plink 1.09:
cd ~/bin
wget https://www.cog-genomics.org/static/bin/plink140918/plink_linux_x86_64.zip
unzip plink_linux_x86_64.zip
cd-

# for denovo vcf, creating a dataset with fake "chr" chromosome designations
cat denovo.vcf | perl -pe 's/locus(\d)(\d+)\t\d+/chr$1\t$2/' >chrom.recode.vcf

# for GATK-based vcf, creating a high-thinned but not re-tabbed dataset with "chr" chromosome designations
hetfilter.pl vcf=gatk_after_vqsr.vcf >hetfiltRaw.vcf
thinner.pl vcf=hetfiltRaw.vcf interval=5000 | perl -pe 's/chrom/chr/g' >thinchrom.vcf
vcftools --vcf thinchrom.vcf --remove clones2remove --remove-filtered-all --max-missing 0.8  --min-alleles 2 --max-alleles 2 --recode --out chrom

# reformatting VCF into plink binary BED format
plink --vcf chrom.recode.vcf --make-bed --out admix

# running ADMIXTURE with
# cross-validation to select K (bash script)
for K in 1 2 3 4 5 6; \
do admixture --cv admix.bed $K | tee log${K}.out; done

grep -h CV log*.out # select K with the minimal value

# listing individuals from vcf file
grep "#CHROM" chrom.recode.vcf | perl -pe 's/\t/\n/g' | grep [0-9] > inds.list

# scp the *.Q and inds.list files to laptop, plot it in R:
# use admixturePlotting.R to plot (will require minor editing - population names)